{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/my-rdkit-env/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda3/envs/my-rdkit-env/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import grid_search\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      ffs are a list of feature-functions.\n",
    "      direc is a directory containing xml files (expected to be train or test).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "\n",
    "    returns: \n",
    "      a sparse design matrix, a dict mapping features to column-numbers,\n",
    "      a vector of target classes, and a list of system-call-history ids in order \n",
    "      of their rows in the design matrix.\n",
    "      \n",
    "      Note: the vector of target classes returned will contain the true indices of the\n",
    "      target classes on the training data, but will contain only -1's on the test\n",
    "      data\n",
    "    \"\"\"\n",
    "    fds = [] # list of feature dicts\n",
    "    classes = []\n",
    "    ids = [] \n",
    "    for datafile in os.listdir(direc):\n",
    "        # extract id and true class (if available) from filename\n",
    "        id_str,clazz = datafile.split('.')[:2]\n",
    "        ids.append(id_str)\n",
    "        # add target class if this is training data\n",
    "        try:\n",
    "            classes.append(util.malware_classes.index(clazz))\n",
    "        except ValueError:\n",
    "            # we should only fail to find the label in our list of malware classes\n",
    "            # if this is test data, which always has an \"X\" label\n",
    "            assert clazz == \"X\"\n",
    "            classes.append(-1)\n",
    "        rowfd = {}\n",
    "        # parse file as an xml document\n",
    "        tree = ET.parse(os.path.join(direc,datafile))\n",
    "        # accumulate features\n",
    "        [rowfd.update(ff(tree)) for ff in ffs]\n",
    "        fds.append(rowfd)\n",
    "        \n",
    "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
    "    return X, feat_dict, np.array(classes), ids\n",
    "\n",
    "\n",
    "def make_design_mat(fds, global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      fds is a list of feature dicts (one for each row).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "       \n",
    "    returns: \n",
    "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
    "        the union of features defined in any of the fds \n",
    "    \"\"\"\n",
    "    if global_feat_dict is None:\n",
    "        all_feats = set()\n",
    "        [all_feats.update(fd.keys()) for fd in fds]\n",
    "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
    "    else:\n",
    "        feat_dict = global_feat_dict\n",
    "        \n",
    "    cols = []\n",
    "    rows = []\n",
    "    data = []        \n",
    "    for i in xrange(len(fds)):\n",
    "        temp_cols = []\n",
    "        temp_data = []\n",
    "        for feat,val in fds[i].iteritems():\n",
    "            try:\n",
    "                # update temp_cols iff update temp_data\n",
    "                temp_cols.append(feat_dict[feat])\n",
    "                temp_data.append(val)\n",
    "            except KeyError as ex:\n",
    "                if global_feat_dict is not None:\n",
    "                    pass  # new feature in test data; nbd\n",
    "                else:\n",
    "                    raise ex\n",
    "\n",
    "        # all fd's features in the same row\n",
    "        k = len(temp_cols)\n",
    "        cols.extend(temp_cols)\n",
    "        data.extend(temp_data)\n",
    "        rows.extend([i]*k)\n",
    "\n",
    "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
    "   \n",
    "\n",
    "    X = sparse.csr_matrix((np.array(data),\n",
    "                   (np.array(rows), np.array(cols))),\n",
    "                   shape=(len(fds), len(feat_dict)))\n",
    "    return X, feat_dict\n",
    "    \n",
    "\n",
    "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
    "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
    "# feature-names to numeric values.\n",
    "## TODO: modify these functions, and/or add new ones.\n",
    "def first_last_system_call_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
    "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
    "      (in other words, it returns a dictionary indicating what the first and \n",
    "      last system calls made by an executable were.)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    first = True # is this the first system call\n",
    "    last_call = None # keep track of last call we've seen\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            if first:\n",
    "                c[\"first_call-\"+el.tag] = 1\n",
    "                first = False\n",
    "            last_call = el.tag  # update last call seen\n",
    "            \n",
    "    # finally, mark last call seen\n",
    "    c[\"last_call-\"+last_call] = 1\n",
    "    return c\n",
    "\n",
    "def system_call_count_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "#             c['num_system_calls'] += 1\n",
    "            if el.tag not in c:\n",
    "                c[el.tag] = 0\n",
    "            else:\n",
    "                c[el.tag] += 1\n",
    "    return c\n",
    "\n",
    "def counts_per_call(tree):\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            if 'el.tag' in c.keys():\n",
    "                c['el.tag'] += 1\n",
    "            else:\n",
    "                c['el.tag'] = 1\n",
    "    return c\n",
    "\n",
    "def counts_per_twocall_window(tree):\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    first = True\n",
    "    second = True\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            #make calls into windows of 2\n",
    "            if first:\n",
    "                firstelem_window = el.tag\n",
    "                first = False\n",
    "            elif second:\n",
    "                secondelem_window = el.tag\n",
    "                second = False\n",
    "                name = firstelem_window+\"-\"+secondelem_window\n",
    "                if name in c.keys():\n",
    "                    c[name] += 1\n",
    "                else:\n",
    "                    c[name] = 1\n",
    "            else:\n",
    "                firstelem_window = secondelem_window\n",
    "                secondelem_window = el.tag\n",
    "                name = firstelem_window+\"-\"+secondelem_window\n",
    "                if name in c.keys():\n",
    "                    c[name] += 1\n",
    "                else:\n",
    "                    c[name] = 1\n",
    "    return c\n",
    "\n",
    "def virus_specific_words(tree):\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    c['NETAPI32.dll.NetUserGetInfo'] = 0\n",
    "    c['urlmon.dll'] = 0\n",
    "    c['urlmon.dll.URLDownloadToFile'] = 0\n",
    "    c['HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft'] = 0\n",
    "    c['HKEY_LOCAL_MACHINE\\Software\\Microsoft'] = 0\n",
    "    c['HKEY_LOCAL_MACHINE\\Keyboard'] = 0\n",
    "    c['HKEY_LOCAL_MACHINE\\SOFTWARE\\Classes'] = 0\n",
    "    c['get_host_by_name'] = 0\n",
    "    c['MSVBVM60.DLL'] = 0\n",
    "    c['C:\\WINDOWS\\system32\\sdra64.exe'] = 0\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            #autorun\n",
    "            if el.tag == 'vm_protect':\n",
    "                if 'target' in el.attrib:\n",
    "                    if el.get('target') == 'NETAPI32.dll.NetUserGetInfo':\n",
    "                        c['NETAPI32.dll.NetUserGetInfo'] = +1\n",
    "        \n",
    "            #lipler classified as a downloader virus, so has a lot of \"urlmon.dll\" and \"downloadtofile\"\n",
    "                    elif el.get('target')[:28] == \"urlmon.dll.URLDownloadToFile\":\n",
    "                        c['urlmon.dll.URLDownloadToFile'] = +1\n",
    "                    elif el.get('target')[:10] == 'urlmon.dll':\n",
    "                        c['urlmon.dll'] = +1\n",
    "                    \n",
    "            #magania --> password stealing virus, so uses cryptography in software class.\n",
    "            elif el.tag == 'open_key':\n",
    "                if 'key' in el.attrib:\n",
    "                    if el.get('key')[:37] == 'HKEY_LOCAL_MACHINE\\Software\\Microsoft':\n",
    "                        c['HKEY_LOCAL_MACHINE\\Software\\Microsoft'] = +1\n",
    "                    elif el.get('key')[:26] == 'HKEY_CURRENT_USER\\Keyboard':\n",
    "                        c['HKEY_LOCAL_MACHINE\\Keyboard'] = +1\n",
    "            elif el.tag == 'query_value':\n",
    "                if 'key' in el.attrib:\n",
    "                    if el.get('key')[:36] == 'HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft':\n",
    "                        c['HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft'] = +1\n",
    "\n",
    "            \n",
    "            #swizzor https://www.microsoft.com/en-us/wdsi/threats/malware-encyclopedia-description?Name=TrojanDownloader%3AWin32%2FSwizzor\n",
    "            if el.tag == 'query_value':\n",
    "                if 'key' in el.attrib:\n",
    "                    if el.get('key')[:35] == 'HKEY_LOCAL_MACHINE\\SOFTWARE\\Classes':\n",
    "                        c['HKEY_LOCAL_MACHINE\\SOFTWARE\\Classes'] = +1\n",
    "            elif el.tag == 'get_host_by_name':\n",
    "                c['get_host_by_name'] = +1\n",
    "                \n",
    "            #VB virus if it has MSVBVM60.DLL it is a VB virus. http://resources.infosecinstitute.com/anatomy-of-a-vb-virus/#gref\n",
    "            elif el.tag == 'load_dll':\n",
    "                if el.get('filename') == 'C:\\WINDOWS\\system32\\MSVBVM60.DLL':\n",
    "                    c['MSVBVM60.DLL'] = 1\n",
    "                    \n",
    "            #zbot creates copy of itself https://www.symantec.com/security_response/writeup.jsp?docid=2010-011016-3514-99&tabid=2\n",
    "            elif el.tag == 'delete_file':\n",
    "                if el.get('srcfile') == \"C:\\WINDOWS\\system32\\sdra64.exe\":\n",
    "                    c['C:\\WINDOWS\\system32\\sdra64.exe'] = +1\n",
    "                    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "outputfile = \"sample_predictions.csv\"  # feel free to change this or take it as an argument\n",
    "\n",
    "# TODO put the names of the feature functions you've defined above in this list\n",
    "ffs = [first_last_system_call_feats, system_call_count_feats, counts_per_call, counts_per_twocall_window, virus_specific_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting training features...\n",
      "done extracting training features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract features\n",
    "print \"extracting training features...\"\n",
    "X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
    "print \"done extracting training features\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(9001)\n",
    "msk = np.random.rand(X_train.shape[0]) < 0.85\n",
    "\n",
    "X_train, X_valid = X_train[msk], X_train[~msk]\n",
    "t_train, t_valid = t_train[msk], t_train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning...\n",
      "done learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO train here, and learn your classification parameters\n",
    "print \"learning...\"\n",
    "learned_W = np.random.random((len(global_feat_dict),len(util.malware_classes)))\n",
    "print \"done learning\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN Hyperparameters\n",
      "  n neighbors: 11\n",
      "  weights: distance\n",
      "  Minkowski power: 1\n",
      "-----\n",
      "train score: 0.9920091324200914\n",
      "validations score: 0.87117903930131\n"
     ]
    }
   ],
   "source": [
    "# kNN Model\n",
    "knn_parameters = {'n_neighbors': range(1,21) + [25, 50, 100, 150], 'weights': ['uniform','distance'], 'p':[1, 2]}\n",
    "knn = grid_search.GridSearchCV(KNeighborsClassifier(), knn_parameters)\n",
    "knn.fit(X_train, t_train)\n",
    "\n",
    "print 'kNN Hyperparameters'\n",
    "print '  n neighbors:', knn.best_estimator_.n_neighbors\n",
    "print '  weights:', knn.best_estimator_.weights\n",
    "print '  Minkowski power:', knn.best_estimator_.p\n",
    "print '-----'\n",
    "print 'train score:', knn.score(X_train,t_train)\n",
    "print 'validations score:', knn.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Hyperparameters\n",
      "  n trees: 50\n",
      "  max depth: 50\n",
      "  max features: None\n",
      "  class weight: None\n",
      "-----\n",
      "train score: 0.9920091324200914\n",
      "validation score: 0.9017467248908297\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rf_parameters = {'n_estimators': [50, 500, 750, 1000], 'max_features': ['sqrt', 'log2', None], \n",
    "                 'max_depth': [4, 25, 50, None], 'class_weight': ['balanced', None]}\n",
    "rf = grid_search.GridSearchCV(RandomForestClassifier(), rf_parameters)\n",
    "rf.fit(X_train, t_train)\n",
    "\n",
    "print 'RF Hyperparameters'\n",
    "print '  n trees:', rf.best_estimator_.n_estimators\n",
    "print '  max depth:', rf.best_estimator_.max_depth\n",
    "print '  max features:', rf.best_estimator_.max_features\n",
    "print '  class weight:', rf.best_estimator_.class_weight\n",
    "print '-----'\n",
    "print 'train score:', rf.score(X_train,t_train)\n",
    "print 'validation score:', rf.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Hyperparameters\n",
      "  k CV folds: 3\n",
      "  fit intercept: True\n",
      "  penalty: l2\n",
      "-----\n",
      "train score: 0.9060121765601218\n",
      "validation score: 0.8602620087336245\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "lr_parameters = {'cv': range(3,11), 'fit_intercept':[True, False], 'penalty': ['l2']}\n",
    "lr = grid_search.GridSearchCV(LogisticRegressionCV(), lr_parameters)\n",
    "lr.fit(X_train, t_train)\n",
    "\n",
    "print 'LR Hyperparameters'\n",
    "print '  k CV folds:', lr.best_estimator_.cv\n",
    "print '  fit intercept:', lr.best_estimator_.fit_intercept\n",
    "print '  penalty:', lr.best_estimator_.penalty\n",
    "print '-----'\n",
    "print 'train score:', lr.score(X_train,t_train)\n",
    "print 'validation score:', lr.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features...\n",
      "done extracting test features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get rid of training data and load test data\n",
    "# del X_train\n",
    "# del t_train\n",
    "# del train_ids\n",
    "print \"extracting test features...\"\n",
    "X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "print \"done extracting test features\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions...\n",
      "done making predictions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO make predictions on text data and write them out\n",
    "print \"making predictions...\"\n",
    "# preds = np.argmax(X_test.dot(learned_W),axis=1)\n",
    "knn_preds = knn.predict(X_test)\n",
    "rf_preds = rf.predict(X_test)\n",
    "lr_preds = lr.predict(X_test)\n",
    "print \"done making predictions\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "print \"writing predictions...\"\n",
    "# util.write_predictions(preds, test_ids, outputfile)\n",
    "util.write_predictions(knn_preds, test_ids, \"knn_predictions.csv\")\n",
    "util.write_predictions(rf_preds, test_ids, \"rf_predictions.csv\")\n",
    "util.write_predictions(lr_preds, test_ids, \"lr_predictions.csv\")\n",
    "print \"done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Hyperparameters\n",
      "  C values: 2\n",
      "-----\n",
      "train score: 0.9375951293759512\n",
      "validation score: 0.8165938864628821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_parameters = {'C':[.25, .5, 1, 2, 5]}\n",
    "svm = grid_search.GridSearchCV(SVC(), svm_parameters)\n",
    "svm.fit(X_train, t_train)\n",
    "\n",
    "print 'SVM Hyperparameters'\n",
    "print '  C values:', svm.best_estimator_.C\n",
    "print '-----'\n",
    "print 'train score:', svm.score(X_train,t_train)\n",
    "print 'validation score:', svm.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_preds = svm.predict(X_test)\n",
    "util.write_predictions(svm_preds, test_ids, \"svm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NN Hyperparameters\n",
      "  activation function: logistic\n",
      "  loss coefficient (alpha value): 0.0001\n",
      "-----\n",
      "train score: 0.9600456621004566\n",
      "validation score: 0.8864628820960698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_parameters = {'activation': ['identity','logistic','tanh','relu'], 'alpha': [.0001, .0001, .001, .01, .1, 1]}\n",
    "nn = grid_search.GridSearchCV(MLPClassifier(), nn_parameters)\n",
    "nn.fit(X_train, t_train)\n",
    "\n",
    "print 'NN Hyperparameters'\n",
    "print '  activation function:', nn.best_estimator_.activation\n",
    "print '  loss coefficient (alpha value):', nn.best_estimator_.alpha\n",
    "print '-----'\n",
    "print 'train score:', nn.score(X_train,t_train)\n",
    "print 'validation score:', nn.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_preds = nn.predict(X_test)\n",
    "util.write_predictions(nn_preds, test_ids, \"nn_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
