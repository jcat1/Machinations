{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import grid_search\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## The following function does the feature extraction, learning, and prediction\\ndef main():\\n    train_dir = \"train\"\\n    test_dir = \"test\"\\n    outputfile = \"sample_predictions.csv\"  # feel free to change this or take it as an argument\\n    \\n    # TODO put the names of the feature functions you\\'ve defined above in this list\\n    ffs = [first_last_system_call_feats, system_call_count_feats]\\n    \\n    # extract features\\n    print \"extracting training features...\"\\n    X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\\n    print \"done extracting training features\"\\n    print\\n    \\n    # TODO train here, and learn your classification parameters\\n    print \"learning...\"\\n    learned_W = np.random.random((len(global_feat_dict),len(util.malware_classes)))\\n    print \"done learning\"\\n    print\\n    \\n    # get rid of training data and load test data\\n    del X_train\\n    del t_train\\n    del train_ids\\n    print \"extracting test features...\"\\n    X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\\n    print \"done extracting test features\"\\n    print\\n    \\n    # TODO make predictions on text data and write them out\\n    print \"making predictions...\"\\n    preds = np.argmax(X_test.dot(learned_W),axis=1)\\n    print \"done making predictions\"\\n    print\\n    \\n    print \"writing predictions...\"\\n    util.write_predictions(preds, test_ids, outputfile)\\n    print \"done!\"\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      ffs are a list of feature-functions.\n",
    "      direc is a directory containing xml files (expected to be train or test).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "\n",
    "    returns: \n",
    "      a sparse design matrix, a dict mapping features to column-numbers,\n",
    "      a vector of target classes, and a list of system-call-history ids in order \n",
    "      of their rows in the design matrix.\n",
    "      \n",
    "      Note: the vector of target classes returned will contain the true indices of the\n",
    "      target classes on the training data, but will contain only -1's on the test\n",
    "      data\n",
    "    \"\"\"\n",
    "    fds = [] # list of feature dicts\n",
    "    classes = []\n",
    "    ids = [] \n",
    "    for datafile in os.listdir(direc):\n",
    "        # extract id and true class (if available) from filename\n",
    "        id_str,clazz = datafile.split('.')[:2]\n",
    "        ids.append(id_str)\n",
    "        # add target class if this is training data\n",
    "        try:\n",
    "            classes.append(util.malware_classes.index(clazz))\n",
    "        except ValueError:\n",
    "            # we should only fail to find the label in our list of malware classes\n",
    "            # if this is test data, which always has an \"X\" label\n",
    "            assert clazz == \"X\"\n",
    "            classes.append(-1)\n",
    "        rowfd = {}\n",
    "        # parse file as an xml document\n",
    "        tree = ET.parse(os.path.join(direc,datafile))\n",
    "        # accumulate features\n",
    "        [rowfd.update(ff(tree)) for ff in ffs]\n",
    "        fds.append(rowfd)\n",
    "        \n",
    "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
    "    return X, feat_dict, np.array(classes), ids\n",
    "\n",
    "\n",
    "def make_design_mat(fds, global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      fds is a list of feature dicts (one for each row).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "       \n",
    "    returns: \n",
    "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
    "        the union of features defined in any of the fds \n",
    "    \"\"\"\n",
    "    if global_feat_dict is None:\n",
    "        all_feats = set()\n",
    "        [all_feats.update(fd.keys()) for fd in fds]\n",
    "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
    "    else:\n",
    "        feat_dict = global_feat_dict\n",
    "        \n",
    "    cols = []\n",
    "    rows = []\n",
    "    data = []        \n",
    "    for i in xrange(len(fds)):\n",
    "        temp_cols = []\n",
    "        temp_data = []\n",
    "        for feat,val in fds[i].iteritems():\n",
    "            try:\n",
    "                # update temp_cols iff update temp_data\n",
    "                temp_cols.append(feat_dict[feat])\n",
    "                temp_data.append(val)\n",
    "            except KeyError as ex:\n",
    "                if global_feat_dict is not None:\n",
    "                    pass  # new feature in test data; nbd\n",
    "                else:\n",
    "                    raise ex\n",
    "\n",
    "        # all fd's features in the same row\n",
    "        k = len(temp_cols)\n",
    "        cols.extend(temp_cols)\n",
    "        data.extend(temp_data)\n",
    "        rows.extend([i]*k)\n",
    "\n",
    "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
    "   \n",
    "\n",
    "    X = sparse.csr_matrix((np.array(data),\n",
    "                   (np.array(rows), np.array(cols))),\n",
    "                   shape=(len(fds), len(feat_dict)))\n",
    "    return X, feat_dict\n",
    "    \n",
    "\n",
    "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
    "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
    "# feature-names to numeric values.\n",
    "## TODO: modify these functions, and/or add new ones.\n",
    "def first_last_system_call_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
    "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
    "      (in other words, it returns a dictionary indicating what the first and \n",
    "      last system calls made by an executable were.)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    first = True # is this the first system call\n",
    "    last_call = None # keep track of last call we've seen\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            if first:\n",
    "                c[\"first_call-\"+el.tag] = 1\n",
    "                first = False\n",
    "            last_call = el.tag  # update last call seen\n",
    "            \n",
    "    # finally, mark last call seen\n",
    "    c[\"last_call-\"+last_call] = 1\n",
    "    return c\n",
    "\n",
    "def system_call_count_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "#             c['num_system_calls'] += 1\n",
    "            if el.tag not in c:\n",
    "                c[el.tag] = 0\n",
    "            else:\n",
    "                c[el.tag] += 1\n",
    "    return c\n",
    "\n",
    "\"\"\"\n",
    "## The following function does the feature extraction, learning, and prediction\n",
    "def main():\n",
    "    train_dir = \"train\"\n",
    "    test_dir = \"test\"\n",
    "    outputfile = \"sample_predictions.csv\"  # feel free to change this or take it as an argument\n",
    "    \n",
    "    # TODO put the names of the feature functions you've defined above in this list\n",
    "    ffs = [first_last_system_call_feats, system_call_count_feats]\n",
    "    \n",
    "    # extract features\n",
    "    print \"extracting training features...\"\n",
    "    X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
    "    print \"done extracting training features\"\n",
    "    print\n",
    "    \n",
    "    # TODO train here, and learn your classification parameters\n",
    "    print \"learning...\"\n",
    "    learned_W = np.random.random((len(global_feat_dict),len(util.malware_classes)))\n",
    "    print \"done learning\"\n",
    "    print\n",
    "    \n",
    "    # get rid of training data and load test data\n",
    "    del X_train\n",
    "    del t_train\n",
    "    del train_ids\n",
    "    print \"extracting test features...\"\n",
    "    X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "    print \"done extracting test features\"\n",
    "    print\n",
    "    \n",
    "    # TODO make predictions on text data and write them out\n",
    "    print \"making predictions...\"\n",
    "    preds = np.argmax(X_test.dot(learned_W),axis=1)\n",
    "    print \"done making predictions\"\n",
    "    print\n",
    "    \n",
    "    print \"writing predictions...\"\n",
    "    util.write_predictions(preds, test_ids, outputfile)\n",
    "    print \"done!\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    main()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "outputfile = \"sample_predictions.csv\"  # feel free to change this or take it as an argument\n",
    "\n",
    "# TODO put the names of the feature functions you've defined above in this list\n",
    "ffs = [first_last_system_call_feats, system_call_count_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting training features...\n",
      "done extracting training features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract features\n",
    "print \"extracting training features...\"\n",
    "X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
    "print \"done extracting training features\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9001)\n",
    "msk = np.random.rand(X_train.shape[0]) < 0.85\n",
    "\n",
    "X_train, X_valid = X_train[msk], X_train[~msk]\n",
    "t_train, t_valid = t_train[msk], t_train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning...\n",
      "done learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO train here, and learn your classification parameters\n",
    "print \"learning...\"\n",
    "learned_W = np.random.random((len(global_feat_dict),len(util.malware_classes)))\n",
    "print \"done learning\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN Hyperparameters\n",
      "  n neighbors: 17\n",
      "  weights: distance\n",
      "  Minkowski power: 1\n",
      "-----\n",
      "train score: 0.984779299847793\n",
      "validations score: 0.8755458515283843\n"
     ]
    }
   ],
   "source": [
    "# kNN Model\n",
    "knn_parameters = {'n_neighbors': range(1,21) + [25, 50, 100, 150], 'weights': ['uniform','distance'], 'p':[1, 2]}\n",
    "knn = grid_search.GridSearchCV(KNeighborsClassifier(), knn_parameters)\n",
    "knn.fit(X_train, t_train)\n",
    "\n",
    "print 'kNN Hyperparameters'\n",
    "print '  n neighbors:', knn.best_estimator_.n_neighbors\n",
    "print '  weights:', knn.best_estimator_.weights\n",
    "print '  Minkowski power:', knn.best_estimator_.p\n",
    "print '-----'\n",
    "print 'train score:', knn.score(X_train,t_train)\n",
    "print 'validations score:', knn.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Hyperparameters\n",
      "  n trees: 1000\n",
      "  max depth: 50\n",
      "  max features: sqrt\n",
      "  class weight: None\n",
      "-----\n",
      "train score: 0.984779299847793\n",
      "validation score: 0.8930131004366813\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rf_parameters = {'n_estimators': [5, 10, 25, 50, 100, 250, 500, 750, 1000], 'max_features': ['sqrt', 'log2', None], \n",
    "                 'max_depth': [2, 4, 10, 25, 50, None], 'class_weight': ['balanced', None]}\n",
    "rf = grid_search.GridSearchCV(RandomForestClassifier(), rf_parameters)\n",
    "rf.fit(X_train, t_train)\n",
    "\n",
    "print 'RF Hyperparameters'\n",
    "print '  n trees:', rf.best_estimator_.n_estimators\n",
    "print '  max depth:', rf.best_estimator_.max_depth\n",
    "print '  max features:', rf.best_estimator_.max_features\n",
    "print '  class weight:', rf.best_estimator_.class_weight\n",
    "print '-----'\n",
    "print 'train score:', rf.score(X_train,t_train)\n",
    "print 'validation score:', rf.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Hyperparameters\n",
      "  k CV folds: 4\n",
      "  fit intercept: True\n",
      "  penalty: l2\n",
      "-----\n",
      "train score: 0.8546423135464232\n",
      "validation score: 0.8493449781659389\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "lr_parameters = {'cv': range(3,11), 'fit_intercept':[True, False], 'penalty': ['l2']}\n",
    "lr = grid_search.GridSearchCV(LogisticRegressionCV(), lr_parameters)\n",
    "lr.fit(X_train, t_train)\n",
    "\n",
    "print 'LR Hyperparameters'\n",
    "print '  k CV folds:', lr.best_estimator_.cv\n",
    "print '  fit intercept:', lr.best_estimator_.fit_intercept\n",
    "print '  penalty:', lr.best_estimator_.penalty\n",
    "print '-----'\n",
    "print 'train score:', lr.score(X_train,t_train)\n",
    "print 'validation score:', lr.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features...\n",
      "done extracting test features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get rid of training data and load test data\n",
    "# del X_train\n",
    "# del t_train\n",
    "# del train_ids\n",
    "print \"extracting test features...\"\n",
    "X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "print \"done extracting test features\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions...\n",
      "done making predictions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO make predictions on text data and write them out\n",
    "print \"making predictions...\"\n",
    "# preds = np.argmax(X_test.dot(learned_W),axis=1)\n",
    "knn_preds = knn.predict(X_test)\n",
    "rf_preds = rf.predict(X_test)\n",
    "lr_preds = lr.predict(X_test)\n",
    "print \"done making predictions\"\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "print \"writing predictions...\"\n",
    "# util.write_predictions(preds, test_ids, outputfile)\n",
    "util.write_predictions(knn_preds, test_ids, \"knn_predictions.csv\")\n",
    "util.write_predictions(rf_preds, test_ids, \"rf_predictions.csv\")\n",
    "util.write_predictions(lr_preds, test_ids, \"lr_predictions.csv\")\n",
    "print \"done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Hyperparameters\n",
      "  C values: 2\n",
      "-----\n",
      "train score: 0.9600456621004566\n",
      "validation score: 0.7663755458515283\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_parameters = {'C':[.25, .5, 1, 2, 5]}\n",
    "svm = grid_search.GridSearchCV(SVC(), svm_parameters)\n",
    "svm.fit(X_train, t_train)\n",
    "\n",
    "print 'SVM Hyperparameters'\n",
    "print '  C values:', svm.best_estimator_.C\n",
    "print '-----'\n",
    "print 'train score:', svm.score(X_train,t_train)\n",
    "print 'validation score:', svm.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_preds = svm.predict(X_test)\n",
    "util.write_predictions(svm_preds, test_ids, \"svm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/my-rdkit-env/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Hyperparameters\n",
      "  activation function: logistic\n",
      "  loss coefficient (alpha value): 0.0001\n",
      "-----\n",
      "train score: 0.928462709284627\n",
      "validation score: 0.8777292576419214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_parameters = {'activation': ['identity','logistic','tanh','relu'], 'alpha': [.0001, .0001, .001, .01, .1, 1]}\n",
    "nn = grid_search.GridSearchCV(MLPClassifier(), nn_parameters)\n",
    "nn.fit(X_train, t_train)\n",
    "\n",
    "print 'NN Hyperparameters'\n",
    "print '  activation function:', nn.best_estimator_.activation\n",
    "print '  loss coefficient (alpha value):', nn.best_estimator_.alpha\n",
    "print '-----'\n",
    "print 'train score:', nn.score(X_train,t_train)\n",
    "print 'validation score:', nn.score(X_valid,t_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_preds = nn.predict(X_test)\n",
    "util.write_predictions(nn_preds, test_ids, \"nn_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
